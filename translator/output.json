{
    "page-1": "10\nCaching with Redis and \nDeployment on Ubuntu \n(DigitalOcean) and Netlify\nIn this chapter, we are going to explore yet another deployment setup – a robust Uvicorn/Gunicorn/Nginx \nsolution that has been tried and tested with Django and other WSGIs but also ASGI web applications. \nThis should give you more than enough choices when starting your next FARM stack project. We will \nalso add a simple caching solution with Redis, relieving MongoDB from some requests that could (and \nshould!) be cached and served directly. Finally, we will deploy our React-based frontend on Netlify, \nanother very popular deployment option, whose simplicity matches its flexibility.\nIn this chapter, we will cover the following topics:\n• Creating an account on DigitalOcean (optional)\n• Preparing our Ubuntu server with Nginx\n• Deployment of a FastAPI instance through Uvicorn, Gunicorn, and Nginx\n• Caching with Redis\n• Creating a free account on Netlify\n• Deployment of the React Frontend on Netlify\nBy the end of this chapter, you should feel confident when it comes to deploying FARM stack-based \napplications on a variety of serving platforms, including a bare-bones Ubuntu (or any Linux) server. \nY ou will be able to recognize where and how to add caching and implement it effortlessly with Redis. \nFinally, with the knowledge of possible deployment solutions, you will be able to make solid decisions \nwhen the time comes to deploy your application.",
    "page-2": "Caching with Redis and Deployment on Ubuntu (DigitalOcean) and Netlify 280\nDeploying FastAPI on DigitalOcean (or really any Linux \nserver!)\nIn this section, we  are going to take our simple analytics application and deploy it on a Ubuntu \nserver  on DigitalOcean  (www.digitalocean.com ) as an Asynchronous Server Gateway \nInterface  (ASGI ) application. We are going to  end up with a pretty robust and customizable setup \nthat includes our development web server – Uvicorn – but also Gunicorn ( https://gunicorn.\norg ), an excellent and robust web server that plays very nicely with Nginx , and a virtual machine \nrunning Ubuntu – a DigitalOcean  droplet. Though in this example we are going to use DigitalOcean, \nthe procedure should apply to any Debian or Ubuntu-based setup; you can try it out on your own \nmachine running Ubuntu. The following instructions rely heavily on the excellent tutorials on setting \nup an Ubuntu server on DigitalOcean  by Brian Boucheron ( https://www.digitalocean.\ncom/community/tutorials/initial-server-setup-with-ubuntu-20-04 ) \nand on deploying an Async Django application by Mason Egger and Erin Glass ( https://www.\ndigitalocean.com/community/tutorials/how-to-set-up-an-asgi-django-\napp-with-postgres-nginx-and-uvicorn-on-ubuntu-20-04 ). Y ou should read them \nas they are very useful and well written!\nImportant Note\nIn this section, we will make heavy use of SSH – the Secure Shell Protocol. SSH is a cryptographic \nprotocol developed for accessing secure network services over insecure networks. If that doesn’t \nmake much sense, do not worry – there are plenty of great resources on basic SSH operations \non the internet. If you are willing to dive a bit deeper into DevOps, you can read the following \nbook: https://www.amazon.com/Mastering-Ubuntu-Server-configuring-\ntroubleshooting/dp/1800564643 . Mastering Ubuntu Server  is an excellent guide on \nthe subject. In the following pages, we will just log into a DigitalOcean droplet, which is nothing \nmore than a remote Ubuntu computer that we will be able to control. While I will show the \nprocedure for deploying a fully functional FastAPI instance on a DigitalOcean droplet, the best \nway to try out this procedure would be to practice on your Ubuntu-based server. If you have a \nspare box (even an older one), install Ubuntu and try connecting to it from your main computer.\nThe deployment procedure will be broken into easy steps.\nDigitalOcean  is one of the leaders in providing cloud computing and Infrastructure as a Service  \n(IaaS ). Users can benefit from different types of virtual machines that can be modeled according to \nour needs. In our case, we just want a solution for hosting our FastAPI server, similar to how we did \nwith Heroku in the previous chapters.\nWhile DigitalOcean  doesn’t provide a completely free tier, it is reasonably cheap to get started (around \n4 USD per month). It has a flexible and scalable system where you can easily scale up or down \naccording to your needs and it offers complete control of the virtual machines – droplets, a  fact that \nbrings us a whole new level of flexibility, a word that we often used in this book. Another advantage ",
    "page-3": "Deploying FastAPI on DigitalOcean (or really any Linux server!)\n 281\nof DigitalOcean  is its excellent community and an endless list of well-written articles on any service \nor setup you may want to achieve, so it represents a good place to start if you are entering the world of \ndeployment, database setup, and so on. Just to be clear, DigitalOcean , as well as its competitors (Linode, \nfor instance) is perfectly able to host our complete full-stack setup – we could install MongoDB on the \nserver as well, add Node.js and Next or a React frontend, and orchestrate everything through Nginx, \na powerful and fast server. In this example, however, we only want to serve our FastAPI instance and \nshowcase a different type of deployment. Follow these steps:\n1. Create an account on DigitalOcean! Head over to the DigitalOcean signup page at https://\ncloud.digitalocean.com/registrations/new  and fill in your data. Y ou can sign \nin with GitHub or Google if you wish, and you can use a referral code if you have one so that \nyou can try out the service for a determined time. Once you submit your data (and once you \nhave some credit to spend – be it from a referral program or after you connect your credit \ncard), you will be able to create your first droplet. \n2. Create a droplet. I have used a Ubuntu 22.04 x64 Ubuntu distribution, the plan is Basic (the \ncheapest), and the CPU options are $4/month with 512 MB/1 CPU (you will have to hit the \nleft arrow to find this plan!). Since I am in Europe, I selected the Frankfurt data center region. \nFinally, to simplify things, I opted for password authentication, so I entered a root password \n(that I am not going to disclose here!). I gave the hostname a name – farmstack . Although \nwe will be using the IP address to access this brand-new machine through SSH, it is useful to \nhave a user-friendly machine name.\nGive DigitalOcean some time to prepare your droplet. After about 30 seconds, you will be able \nto click on the lefthand menu under Droplets  and you will be taken to a page that displays \ninformation about your droplet. Y ou now have a Ubuntu-based server under your control! \n3. To verify that you are indeed able to log in as root on your brand-new machine, click on the IP \naddress of the  droplet to copy it, open Cmder  (or whatever shell you have been using this whole \ntime) on Windows or a bash/shell if you are on Linux or macOS, and try to access the droplet:\nssh root@<your_IP_address_that_you_just_copied>\n4. Cmder  will kindly inform  you that the authenticity of the host cannot be established, which is \nnormal at this stage, and ask you if you want to continue connecting. Type yes ; you will be \ngreeted with a shell that should read as follows:\nroot@farmstack:~#\n5. It is good practice to create a new user account that will have all the necessary privileges so that \nwe don’t use the root account for our web hosting. Let’s create an account called farmuser :\nadduser farmuser",
    "page-4": "Caching with Redis and Deployment on Ubuntu (DigitalOcean) and Netlify 282\n6. Y ou will be asked to provide a password (twice), a name, and some other information, such \nas a room number (!). It is important to remember the password! This newly created user will \nneed to be able to perform various administrative tasks, so we should grant them adequate \nprivileges. In the same SSH session, type the following:\nusermod -aG sudo farmuser\nAfter this, when we log in as farmuser , we will be able to just type sudo  before performing \nactions that require superuser powers.\nWe will make use of the UFW  firewall to make sure that only certain types of connections to \nour server are permitted. There are different options when it comes to DigitalOcean’s firewalls, \nbut this should be more than enough and easy to set up on different machines. Things may get \ntricky, though – we need to make sure that when we leave our SSH root shell, we will be able \nto get back in with our farmuser  account!\n7. To be sure that OpenSSH is allowed to access the machine, type the following:\nufw allow OpenSSH\n8. Y ou should see a message saying rules updated . Now, let’s enable ufw  and check its status:\nufw enable\nufw status\nThe preceding  commands should warn you that they may  disrupt existing SSH connections; \nconfirm the first one anyway. The second should just inform you that the service is active. \n9. Great. Now, Keep the SSH session alive  and open a new terminal so that we can test our \nconnection with our regular yet highly privileged farmuser :\nssh farmuser@<your_IP_address_that_you_just_copied>\nY ou should be greeted with a prompt; that is, farmuser@farmstack:~$ . That’s great – \nnow, we can proceed with this (regular) user and use sudo  when we need to do tricky stuff!\nIt is time to update our Ubuntu packages and add some more. Logged in as farmuser  (or \nwhatever your regular, non-root username was), issue the following command:\nsudo apt update\nsudo apt install python3-venv nginx curl\nsudo  will prompt you for a password – your regular farmuser  password – so kindly provide \nit. Apart from Python 3, we are installing Nginx , our powerful web server and reverse proxy \nsolution, and curl  (to test our API service locally).",
    "page-5": "Deploying FastAPI on DigitalOcean (or really any Linux server!)\n 283\nNow, we are entering the second, project-related phase of our deployment. It is time to create a virtual \nenvironment, just like we did numerous times during the development phase. This is a bare-bones \nserver, so we have to do everything  manually. There is no helpful guiding hand like there was with \nHeroku  or Vercel . Follow these steps:\n1. Let’s create a directory called apiserver  in our home folder and cd into it (you can always \nsee where you are currently located with PWD!):\nmkdir ~/apiserver\ncd ~/apiserver\n2. Now, let’s create a Python 3 environment:\npython3 -m venv venv \n3. After the setup has finished, go ahead and activate this environment with the following command:\nsource venv/bin/activate\nY ou should see venv  prepending the command prompt.\n4. It is time to grab the address of the GitHub repository that you created for the backend and \nchange the directory to our /apiserver . Now, clone the GitHub repo inside to get all the code:\ngit clone <your repo address>\nThis will create a folder with the same name as the repository – in my case, it is a bit cumbersome: \nFARM-chapter9-backend . Cloning the code from the repo will not copy the .env  file \nwith the necessary keys for MongoDB and Sendgrid (and Cloudinary in the previous app). \n5. Although we could set the environment variables manually through the shell, we are just going \nto blatantly copy them using the secure copy scp  command. Make sure you’re in your local \ncomputer’s /backend  folder and take note of the remote folder. Then, issue the following \ncommand:\nscp .env farmuser@207.154.254.114:~/apiserver/FARM-\nchapter9-backend\n6. Now try out the ls command to make sure that the folder with the code is indeed there, but \nkeep in mind that the .env  file will not be shown! Y ou will have to use something such as \nnano .env  to verify that the file is indeed there and that it contains the necessary information. \nIf you don’t want to mess with scp , you can just create and type in the .env  file using nano \n– the powerful command-line text editor provided by Linux systems.\n7. Once the code is in the Ubuntu droplet, cd into the directory and install all the dependencies \nwith the following command:\npip install -r requirements.txt",
    "page-6": "Caching with Redis and Deployment on Ubuntu (DigitalOcean) and Netlify 284\nImportant Note \nAfter committing the code for the backend, you should update the requirements.txt  \nfile by typing pip freeze > requirements.txt , while being within the activated \nvirtual environment on your local machine. This file should then be committed to GitHub – it \nwill be our magic ingredient for recreating the same virtual environment on other machines, \nincluding our droplet!\n8. Once the dependencies have been installed, we can test our application with the standard \nUvicorn command:\nuvicorn main:app –reload\nThe prompt should inform you that Uvicorn is running on http://127.0.0.1:8000  \nbut that we cannot access it yet from the outside. \n9. Stop the server with Ctrl + C. To be able to test that the API is working, we have to disable our \nUFW firewall. To do that, you have to sudo  your way through it:\nsudo ufw disable\nNotice\nThis is a dangerous practice – a bit like leaving your front door open.\nNow, if you try to rerun the Uvicorn  server, you should be able to access your API with a REST client \nor a browser at the IP address of your droplet, on port 8000 ! So far, we are only trying out what we \nhave  been doing throughout this book on DigitalOcean. Now, it is time to introduce Gunicorn.\nImportant Note\nGunicorn  is a mature and battle-tested WSGI Python server for UNIX. It is often used in \nconjunction with Uvicorn since it is  highly configurable and able to handle Uvicorn workers \nefficiently. The Uvicorn documentation itself recommends a setup that includes Gunicorn and \nNginx and that is exactly what we are going to do! Gunicorn is an interesting and  powerful project \nin its own right and its documentation is is a useful read ( https://gunicorn.org/ ).\nLet’s build our deployment  now. Follow these steps:\n1. Install gunicorn  with a simple call to pip :\npip install gunicorn",
    "page-7": "Deploying FastAPI on DigitalOcean (or really any Linux server!)\n 285\n2. After installing gunicorn , we can start our API server with the following command (while \nstaying in the source code directory!):\ngunicorn --bind 0.0.0.0:8000 main:app -w 4 -k uvicorn.\nworkers.UvicornWorker\nThe preceding command starts a gunicorn  server with four uvicorn  workers. Gunicorn provides \nalso load balancing functionality for our Uvicorn servers – an async request that might be taking \na bit too long won’t hog up the system. Now, we can test our app on port 8000 . \nNow, we are going to use Linux’s powerful systemd  service and socket files to make the server \nstart and stop programmatically.\nImportant Note\nsystemd  is a process and system  manager for Linux systems. If you wish to get to know its \ncapabilities and functionalities, I can recommend (another) very useful article from the \nDigitalOcean knowledge database: https://www.digitalocean.com/community/\ntutorials/systemd-essentials-working-with-services-units-and-\nthe-journal . Again, in these pages, we will only explain the commands that we will be \nusing – starting, stopping, and enabling and disabling services, servers, and so on.\n3. We are going to have to use a bit of nano , the command-line text editor of choice for the \nmajority of Linux distributions. Stop the gunicorn  server with Crtl + C and deactivate the \nvirtual environment with a simple deactivate . The prepended venv  should be gone. \n4. Now, let’s create a gunicorn  socket. Sockets are simply communication points on the same or \ndifferent computers that enable systems to exchange data. When we create a Gunicorn socket, \nit is just a way of telling the system that the created socket can be used to access data that the \nserver will provide:\nsudo nano /etc/systemd/system/gunicorn.socket\nThe file’s content should be as follows (fully adapted from the aforementioned ASGI Django guide):\n[Unit]\nDescription=gunicorn socket\n[Socket]\nListenStream=/run/gunicorn.sock\n[Install]\nWantedBy=sockets.target",
    "page-8": "Caching with Redis and Deployment on Ubuntu (DigitalOcean) and Netlify 286\n5. To leave nano, just type Ctrl + X and type yes  when asked to confirm. The filename should \nremain the same as what we gave it initially.\n6. Now, we are going to  create the gunicorn.service  file. Again, fire up nano  with the \nfollowing command:\nsudo nano /etc/systemd/system/gunicorn.service\n7. Begin typing the  following:\n[Unit]\nDescription=gunicorn daemon\nRequires=gunicorn.socket\nAfter=network.target\n[Service]\nUser=farmuser\nGroup=www-data\nWorkingDirectory=/ home/farmuser/apiserver/FARM-chapter9-\nbackend\nExecStart =/home/farmuser/apiserver/venv/bin/gunicorn  \\\n          --access-logfile - \\\n          -k uvicorn.workers.UvicornWorker \\\n          --workers 3 \\\n          --bind unix:/run/gunicorn.sock \\\n          main:app\n[Install]\nWantedBy=multi-user.target\nI have highlighted the essential parts and paths that you should triple-check before saving. It \nis important to emphasize that the working directory is the directory hosting our code , while \nexecstart  is referring to the virtualenv  directory. In our case, they are side by side \ninside the apiserver  folder! This should be enough for systemd . \n8. Save the file and let’s try  it out. Start and enable the newly created gunicorn  socket with the \nfollowing commands: \nsudo systemctl start gunicorn.socket\nsudo systemctl enable gunicorn.socket",
    "page-9": "Deploying FastAPI on DigitalOcean (or really any Linux server!)\n 287\n9. If everything went right, there shouldn’t be any errors. Y ou should, however, check the status \nof the socket:\nsudo systemctl status gunicorn.socket\n10. Y ou should also check for the existence of the gunicorn.sock  file:\nfile /run/gunicorn.sock\n11. Now, activate the socket:\nsudo systemctl status gunicorn\n12. With that, we should be able to (finally!) test our API with curl :\ncurl --unix-socket /run/gunicorn.sock localhost/cars/all\nY ou should get a bunch of cars flooding the terminal since we’ve hit our cars  endpoint!\nWe’re nearly there, hang on! Now, we will use Nginx to route the incoming traffic. Follow these steps:\nImportant Note\nNginx is an extremely powerful, reliable, and fast web server, load balancer, and proxy server. At \nits most basic, Nginx reads its configuration and, based on this information, decides what to do \nwith each request that it encounters – it can simultaneously handle multiple websites, multiple \nprocesses, and the most diverse configurations that you throw at it. Y ou may have a bunch of \nstatic files, images, and documents in one location on the server, a Node.js API managed by \nPM2, a Django or Flask website, and maybe a FastAPI instance all at once. With the proper \nconfiguration, Nginx will be able to effortlessly take care of this mess and always serve the right \nresource to the right client. At least some  basic knowledge of how Nginx operates can be a \nvery useful tool to have under your belt, and the nginx.org  website is a great place to start.\n13. Nginx operates  in server blocks, so let’s create one for our apiserver :\nserver {\n    listen 80;\n    server_name <your droplet's IP address>\n    location = /favicon.ico { access_log off; log_not_\nfound off; }\n    \n    location / {\n        include proxy_params;\n        proxy_pass http://unix:/run/gunicorn.sock;",
    "page-10": "Caching with Redis and Deployment on Ubuntu (DigitalOcean) and Netlify 288\n    }\n}\nOnce you get used to Nginx’s server block syntax, you will be serving websites (or processes, \nto be precise) in no time. In the preceding code, we instructed Nginx to listen on the default \nport (80) for our machine (IP address) and to redirect all traffic to our Unix Gunicorn socket!\n14. Now, enable the file by copying it to the sites-enabled  folder of Nginx, as follows:\nsudo ln -s /etc/nginx/sites-available/myproject /etc/\nnginx/sites-enabled\nThere is a very handy command that allows us to check if the Nginx configuration is valid:\nsudo nginx -t\n15. If Nginx is not  complaining, we can restart it by typing the following command; then, we \nshould be good to go:\nsudo systemctl restart nginx\n16. The last thing we must do is set up the ufw  firewall again, allow Nginx to pass through, and \nclose port 8000  by removing the rule that allowed it:\nsudo ufw delete allow 8000\nsudo ufw allow 'Nginx Full'\nCongratulations! Y ou are now serving your API through a robust setup that consists of Uvicorn, \nGunicorn, and Nginx. With this setup, we have a plethora of options. Y ou could serve static files (images, \nstylesheets, or documents) blazingly fast through Nginx. Y ou could also set up a Next.js project and \nmanage it through PM2 ( https://pm2.keymetrics.io/ ), a powerful Node.js process manager. \nWe will stop here, although there are many – not so complicated – steps to go through before we have \na production-ready system.\nAdding caching with Redis\nRedis is among the  top technologies when it comes to NoSQL data storage options, and it is very \ndifferent from MongoDB. Redis is an in-memory data structure store, and it can be used as a database, \ncache, message broker, and also for streaming. Redis provides simple data structures – hashes, lists, \nstrings, sets, and more –and enables scripting with the Lua language. While it can be used as a primary \ndata store, it is often used for caching or running analytics and similar tasks. Since it is built to be \nincredibly fast (much faster than MongoDB, to be clear), it is ideal for caching database or data store \nqueries, results of complex computations, API calls, and managing the session state. MongoDB, on \nthe other hand, while being fast and flexible, if it scales sufficiently, could slow down a bit. Bearing \nin mind that we often (as is the case in this chapter) host MongoDB on one server (Atlas Cloud) and ",
    "page-11": "Adding caching with Redis\n 289\nour FastAPI code on another one (DigitalOcean or Heroku), latency also might affect the response \ntimes. Imagine if we wanted to perform a complex aggregation instead of the simple ones that we have \ncreated in this chapter. By throwing in some data science, such as algorithms with interpolations or \nmachine learning algorithms, we could be in trouble should our website become popular (and it will!). \nCaching to the rescue! What is caching? It is a really simple concept that has been around for decades \n– the basic idea is to store some frequently requested data (from a Mongo database, in our case) in \nsome type of temporary storage for some time until it expires. The first user requesting said resource \n(a list of cars) will have to wait for the whole query to complete and will get the results. These results \nwill then automatically be added to this temporary storage (in our case, Redis, the Usain Bolt of \ndatabases) and served to all subsequent requests for the same data. By the same data, we usually imply \nthe same endpoint. This process persists until the data stored in Redis (or any other caching solution \nthat you may use) expires – if valid data is not found in the cache, the real database call is made again \nand the process repeats. \nThe expiry time is of crucial importance here – in our case, if we are working with a car-selling company, \nwe can be generous with caching and extend the expiry period to 10 minutes or even more. In more \ndynamic applications, such as forums or similar conversational environments, a much lower expiry \ntime would be mandatory to preserve functionality.\nInstalling Redis on Linux is quite simple, while on Windows it is not officially supported. Y ou could \nfollow the official guide for installing Redis for development purposes on Windows ( https://\nredis.io/docs/getting-started/installation/install-redis-on-windows/ ) \nbut that is beyond the scope of our application. We will, however, install Redis on our DigitalOcean \nLinux box and add caching to our FastAPI application!\nConnect to your DigitalOcean box (or to your Linux system of choice – if you are developing on \nLinux or Mac, you should install it there as well) by following the steps from this chapter, while using \nSSH from a terminal:\n1. Now, install Redis by typing the following command:\nsudo apt install redis-server\nImportant Note \nIn a production environment, you should secure your Redis server with a disgustingly long \npassword. Since Redis is fast, an attacker could potentially run hundreds of thousands of \npasswords in mere seconds against it during a brute-force attack. Y ou should also disable or \nrename some potentially dangerous Redis commands. In these pages, we are only showing how \nto add a bare-bones, not-secured Redis instance to our setup.",
    "page-12": "Caching with Redis and Deployment on Ubuntu (DigitalOcean) and Netlify 290\n2. Now, we should restart the Redis service. Although it should happen automatically, let’s make \nsure by typing the following command:\nsudo systemctl restart redis.service\n3. Test it by typing the following command to see if it is working:\nsudo systemctl status redis\nThe Terminal will send an ample response, but what you are looking for is the green word \nActive  (running). It should also be started automatically with every reboot – so we get that \ngoing for us, which is nice.\n4. The traditional way to test that Redis is responding is to start the client:\nredis-cli\nThen, in the Redis shell, type ping .\nRedis should respond with pong  and the prompt should say 127.0.0.1:6379 . This means that Redis \nis running on localhost (the Linux server) on port 6379 . Remember this address, or better, write it \ndown somewhere (I know, I know). We are going to need it for our FastAPI server.\nThere are many ways to make Redis talk to Python, but here, we will opt for a simple module aptly \nnamed Fastapi-cache  (https://github.com/long2ice/fastapi-cache ). Now, we will \nhave to edit our backend code in the /backend  folder. When we’re done, we will push the changes \nto GitHub and repeat the deployment procedure. Or, if you just want to quickly try out the caching, \nyou could edit the files on DigitalOcean  directly by navigating to the directory and using nano .\nAnyway, activate the virtual environment of your choice and install the package and aioredis  (the \nasync Python Redis driver):\npip install fastapi-cache2 aioredis\nNow, our FastAPI project structure dictates which files need to be updated. We need to update our \nmain.py  file and add the following imports:\nimport aioredis\nfrom fastapi_cache import FastAPICache\nfrom fastapi_cache.backends.redis import RedisBackend\nThen, we need to update our startup event handler:\n@app.on_event(\"startup\")\nasync def startup_db_client():\n    app.mongodb_client = AsyncIOMotorClient(DB_URL)",
    "page-13": "Adding caching with Redis\n 291\n    app.mongodb = app.mongodb_client[DB_NAME]\n    redis = aioredis.from_url(\n        \"redis://localhost:6379\", encoding=\"utf8\", decode_\n            responses=True\n    )\n    FastAPICache.init(RedisBackend(redis), prefix=\"fastapi-\n        cache\")\nThe code makes sense – we’re getting a Redis client, just like we did with Mongo, and we are passing \nthe URL and a couple of (suggested) settings. Finally, we initialized the FastAPICache . Now, we need \nto add the caching decorator to our endpoints, which are located in the /routers/cars.py  file. \nWe will add one import:\nfrom fastapi_cache.decorator import cache\nNow, we can decorate  the routes that we wish to cache (only GET requests, but that’s all we have in \nthis project really). Edit the /sample  route:\n@router.get(\"/sample/{n}\", response_description=\"Sample of N \ncars\")\n@cache(expire=60)\nasync def get_sample(n: int, request: Request):\n    query = [\n        {\"$match\": {\"year\": {\"$gt\": 2010}}},\n        {\n            \"$project\": {\"_id\": 0,}\n        },\n        {\"$sample\": {\"size\": n}},\n        {\"$sort\": {\"brand\": 1, \"make\": 1, \"year\": 1}},\n    ]\n    full_query = request.app.mongodb[\"cars\"].aggregate(query)\n    results = [el async for el in full_query]\n    return results\nThis route is now cached, which means that when it’s hit, it will provide a sample of size N and then, \nfor all subsequent requests in the next 60 seconds, it will send the same cached response. Go ahead \nand try it out, either on your DigitalOcean API or local environment, depending on where you \nimplemented caching. Try hitting  the API for 1 minute – you should always get the same result until the \ncache expires. Congratulations – you have just added a top-of-the-class caching solution to your API!",
    "page-14": "Caching with Redis and Deployment on Ubuntu (DigitalOcean) and Netlify 292\nDeploying the Frontend on Netlify\nSimilar to Vercel, Netlify is one of the  top companies providing  services for static web hosting and \nserverless computing, but also a rather simple CMS and goodies such as form handling. It is widely \nregarded as one of the best solutions for hosting JAMStack  websites and its content delivery network  \n(CDN ) can speed up the hosted websites significantly. It is  also one of the easiest ways to host a React \napplication. This is what we are going to use it for in this section.\nAfter logging in with your Google or GitHub account, you will be presented with a screen that offers \nyou the possibility to deploy a new project:\nFigure 10.1 – The Netlify Add New Site button\nNext, you will be asked whether you are importing an existing project (yes!); you should choose your \nReact frontend project from GitHub. If you logged in with GitHub, you won’t have to authorize Netlify \nagain – if not, please authorize it:\nFigure 10.2 – The Import and existing project page on Netlify\n",
    "page-15": "Deploying the Frontend on Netlify\n 293\nAfter browsing through your GitHub projects, point Netlify to the React frontend and leave all the \ndefaults that Netlify was able to cleverly infer from the project. Y ou will be presented with a page on \nwhich you could potentially modify any deployment setting, but we will limit ourselves to just adding \na single environment variable. Y ou’ve guessed it – it’s the handy REACT_APP_API_URL !\nFigure 10.3 – Netlify’s pre-deployment setting page\n",
    "page-16": "Caching with Redis and Deployment on Ubuntu (DigitalOcean) and Netlify 294\nY ou will have to add just one variable in the advanced settings: you’ve guessed it – REACT_APP_API_\nURL . Create  a New variable  by hitting the respective button and name it REACT_APP_API_URL . \nThe value should be https://yourdomain.com :\nFigure 10.4 – Adding the new environment variable in Netlify\nAfter some time, maybe a minute or so, you will have your deployment ready for the world to see! In \ncase of any problems (and there will be problems), you should inspect Netlify’s deployment console \nand watch for hiccups.\nY our React frontend with all its fancy charts and fast pagination will now be served from Netlify’s fast \ncontent delivery network  (CDN ) while operating on a FastAPI (cached) backend served by Nginx \non DigitalOcean. Throw in our previously explored Heroku and Vercel deployments and you have a \nlot of options to start tinkering!\nThis doesn’t mean that these are your only deployment options! A popular and rock-solid choice is \nto use a Docker container and containerize your application (together or separately) and provide this \nDocker image to some of the giants – Amazon Web Services  (AWS ), Microsoft Azure, or Google \nApp Engine. This type of deployment isn’t much different from the Heroku deployment, although it \nrequires creating the proper type of account and setting the environment the right way. These solutions \nalso tend to have higher upfront costs.\n",
    "page-17": "Summary\n 295\nSummary\nIn this chapter, we added a very simple yet powerful caching solution based on Redis – an incredibly \npowerful product in its own right. We went through the tedious but often necessary procedure of \nhosting the API on a Ubuntu server behind Gunicorn and the mighty Nginx – a server that offers \nso much flexibility and configurability that it simply has to be put in the conversation of the FARM \nstack. As a bonus, we explored yet another cheap (well, free) frontend hosting option – Netlify – which \noffers premiere continuous deployment and plays very nicely with all our frontend solutions, be it \nplain React or Next.js or maybe, in the future, React-Remix. Now, you should feel confident enough \nto dive head-first into your next project and peruse the numerous options that FastAPI, React, and \nMongoDB have to offer by playing nicely with each other.\nIn the next chapter, we will try to address some of the best practices that pertain to the components of \nthe stack in every project, as well as some topics that we haven’t touched on but are equally important, \nsuch as testing, using static templates with Jinja2, site monitoring, and more."
}